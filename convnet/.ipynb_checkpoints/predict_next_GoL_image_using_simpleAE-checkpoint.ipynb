{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import os\n",
    "import chainer\n",
    "from chainer.dataset import convert\n",
    "import cv2\n",
    "import net_conv\n",
    "import hickle as hkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data input-output variables\n",
    "# path to dataset hkl files\n",
    "PATH = '../datasets/gol/'\n",
    "PATH_glider = '../datasets/glider/'\n",
    "# output path\n",
    "out = 'results/gol'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training, validation and test data\n",
    "X_train = hkl.load(PATH + 'X_train.hkl')\n",
    "X_train = X_train / 255.\n",
    "X_train = X_train.astype('float32')\n",
    "#?sources = hkl.load(PATH + 'sources_train.hkl')\n",
    "\n",
    "X_val = hkl.load(PATH + 'X_val.hkl')\n",
    "X_val = X_val / 255.\n",
    "X_val = X_val.astype('float32')\n",
    "#?sources_val = hkl.load(PATH + 'sources_val.hkl')\n",
    "\n",
    "X_test = hkl.load(PATH + 'X_test.hkl')\n",
    "X_test = X_test / 255.\n",
    "X_test = X_test.astype('float32')\n",
    "#?sources_test = hkl.load(PATH + 'sources_test.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-38143bea4d10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Example of input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# Example of input data\n",
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size_0 = X_train[0].shape[0]\n",
    "input_size_1 = X_train[0].shape[1]\n",
    "gpu = -1\n",
    "dimz = 10\n",
    "batchsize = 16\n",
    "epoch = 1000\n",
    "initmodel = ''\n",
    "resume = ''\n",
    "is_test = False #'store_true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: -1\n",
      "# dim z: 10\n",
      "# Minibatch-size: 16\n",
      "# epoch: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('GPU: {}'.format(gpu))\n",
    "print('# dim z: {}'.format(dimz))\n",
    "print('# Minibatch-size: {}'.format(batchsize))\n",
    "print('# epoch: {}'.format(epoch))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize / Resume\n",
    "if initmodel:\n",
    "    chainer.serializers.load_npz(initmodel, model)\n",
    "if resume:\n",
    "    chainer.serializers.load_npz(resume, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data list\n",
    "train = list()\n",
    "for idx in range(len(X_train)-1):\n",
    "    if sources[idx] == sources[idx+1]:\n",
    "        train.append((np.transpose(X_train[idx], (2,0,1)), np.transpose(X_train[idx+1], (2,0,1))))\n",
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation data list\n",
    "val = list()\n",
    "for idx in range(len(X_val)-1):\n",
    "    if sources2[idx] == sources2[idx+1]:\n",
    "        val.append((np.transpose(X_val[idx], (2,0,1)), np.transpose(X_val[idx+1], (2,0,1))))\n",
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test data list\n",
    "test = list()\n",
    "for idx in range(len(X_test)):\n",
    "    test.append(np.transpose(X_test[idx], (2,0,1)))\n",
    "test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train)\n",
    "random.shuffle(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    train, _ = chainer.datasets.split_dataset(train, 100)\n",
    "    val, _ = chainer.datasets.split_dataset(val, 100)\n",
    "train_count = len(train)\n",
    "val_count = len(val)\n",
    "\n",
    "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "val_iter = chainer.iterators.SerialIterator(val, batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0xb275683c8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = net_conv.ConvAE_mini(input_size_0=input_size_0, input_size_1=input_size_1, channel_size=3, n_filters=10, n_latent=dimz, filter_size=3, activation='relu')\n",
    "# Setup an optimizer\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "train mean loss=519.4925\n",
      "test mean loss=29718.782248263888\n",
      "2\n",
      "train mean loss=259.1504861111111\n",
      "test mean loss=14399.568684895834\n",
      "3\n",
      "train mean loss=130.03690972222222\n",
      "test mean loss=7526.9036241319445\n",
      "4\n",
      "train mean loss=99.30638888888889\n",
      "test mean loss=5504.902571614583\n",
      "5\n",
      "train mean loss=82.37760416666667\n",
      "test mean loss=4778.95458984375\n",
      "6\n",
      "train mean loss=81.05361111111111\n",
      "test mean loss=4439.381472439236\n",
      "7\n",
      "train mean loss=70.81510416666667\n",
      "test mean loss=4231.436100260416\n",
      "8\n",
      "train mean loss=69.66587239583333\n",
      "test mean loss=4094.926736111111\n",
      "9\n",
      "train mean loss=68.83686631944444\n",
      "test mean loss=3993.2061577690974\n",
      "10\n",
      "train mean loss=64.25390190972222\n",
      "test mean loss=3917.165261501736\n",
      "11\n",
      "train mean loss=66.50163628472222\n",
      "test mean loss=3866.642377387153\n",
      "12\n",
      "train mean loss=89.76614583333334\n",
      "test mean loss=3826.6127983940974\n",
      "13\n",
      "train mean loss=65.7190234375\n",
      "test mean loss=3792.4627332899304\n",
      "14\n",
      "train mean loss=58.578815104166665\n",
      "test mean loss=3763.8424207899307\n",
      "15\n",
      "train mean loss=67.40148003472223\n",
      "test mean loss=3741.2982530381946\n",
      "16\n",
      "train mean loss=66.36851128472222\n",
      "test mean loss=3722.1893391927083\n",
      "17\n",
      "train mean loss=53.49005208333333\n",
      "test mean loss=3704.4796495225696\n",
      "18\n",
      "train mean loss=56.83572916666667\n",
      "test mean loss=3689.0147081163195\n",
      "19\n",
      "train mean loss=65.42580295138889\n",
      "test mean loss=3673.915706380208\n",
      "20\n",
      "train mean loss=63.77600260416666\n",
      "test mean loss=3659.251388888889\n",
      "21\n",
      "train mean loss=72.09531684027777\n",
      "test mean loss=3644.0499131944443\n",
      "22\n",
      "train mean loss=58.999583333333334\n",
      "test mean loss=3629.5552897135417\n",
      "23\n",
      "train mean loss=61.37853298611111\n",
      "test mean loss=3615.2247124565974\n",
      "24\n",
      "train mean loss=71.44936197916667\n",
      "test mean loss=3600.4117241753474\n",
      "25\n",
      "train mean loss=52.09631076388889\n",
      "test mean loss=3584.6333604600695\n",
      "26\n",
      "train mean loss=56.325703125\n",
      "test mean loss=3562.2407280815974\n",
      "27\n",
      "train mean loss=61.668268229166664\n",
      "test mean loss=3541.657297092014\n",
      "28\n",
      "train mean loss=59.826137152777775\n",
      "test mean loss=3523.4629177517363\n",
      "29\n",
      "train mean loss=60.738719618055555\n",
      "test mean loss=3505.1475531684027\n",
      "30\n",
      "train mean loss=52.97219618055556\n",
      "test mean loss=3487.333566623264\n",
      "31\n",
      "train mean loss=59.82644097222222\n",
      "test mean loss=3469.153759765625\n",
      "32\n",
      "train mean loss=52.86704427083333\n",
      "test mean loss=3449.434152560764\n",
      "33\n",
      "train mean loss=58.58756076388889\n",
      "test mean loss=3428.361083984375\n",
      "34\n",
      "train mean loss=60.307200520833334\n",
      "test mean loss=3406.3631510416667\n",
      "35\n",
      "train mean loss=56.6487890625\n",
      "test mean loss=3383.4949652777777\n",
      "36\n",
      "train mean loss=65.25040364583333\n",
      "test mean loss=3358.671088324653\n",
      "37\n",
      "train mean loss=56.118389756944445\n",
      "test mean loss=3331.4217881944446\n",
      "38\n",
      "train mean loss=63.74333333333333\n",
      "test mean loss=3304.5317654079863\n",
      "39\n",
      "train mean loss=54.05344184027778\n",
      "test mean loss=3273.6917914496526\n",
      "40\n",
      "train mean loss=51.65197048611111\n",
      "test mean loss=3239.608203125\n",
      "41\n",
      "train mean loss=56.73911892361111\n",
      "test mean loss=3204.279709201389\n",
      "42\n",
      "train mean loss=53.25832899305556\n",
      "test mean loss=3169.304931640625\n",
      "43\n",
      "train mean loss=53.8071484375\n",
      "test mean loss=3132.69208984375\n",
      "44\n",
      "train mean loss=56.03766059027778\n",
      "test mean loss=3095.380072699653\n",
      "45\n",
      "train mean loss=50.17310329861111\n",
      "test mean loss=3055.9662489149305\n",
      "46\n",
      "train mean loss=49.20813802083333\n",
      "test mean loss=3016.776177300347\n",
      "47\n",
      "train mean loss=52.244053819444446\n",
      "test mean loss=2977.934380425347\n",
      "48\n",
      "train mean loss=51.70816840277778\n",
      "test mean loss=2939.348356119792\n",
      "49\n",
      "train mean loss=50.21072916666667\n",
      "test mean loss=2895.005322265625\n",
      "50\n",
      "train mean loss=47.68256510416666\n",
      "test mean loss=2851.6597819010417\n",
      "51\n",
      "train mean loss=48.22113715277778\n",
      "test mean loss=2809.606146918403\n",
      "52\n",
      "train mean loss=50.69112413194444\n",
      "test mean loss=2767.860183376736\n",
      "53\n",
      "train mean loss=43.33535590277778\n",
      "test mean loss=2724.0683322482637\n",
      "54\n",
      "train mean loss=45.692057291666664\n",
      "test mean loss=2679.2982476128473\n",
      "55\n",
      "train mean loss=45.20634548611111\n",
      "test mean loss=2616.3094563802083\n",
      "56\n",
      "train mean loss=42.09649739583333\n",
      "test mean loss=2563.7229112413193\n",
      "57\n",
      "train mean loss=44.81936631944444\n",
      "test mean loss=2512.0179144965277\n",
      "58\n",
      "train mean loss=40.421184895833335\n",
      "test mean loss=2462.289914279514\n",
      "59\n",
      "train mean loss=40.73015625\n",
      "test mean loss=2416.89248046875\n",
      "60\n",
      "train mean loss=38.02071614583333\n",
      "test mean loss=2374.9863009982637\n",
      "61\n",
      "train mean loss=38.860881076388885\n",
      "test mean loss=2333.632695855035\n",
      "62\n",
      "train mean loss=41.7015625\n",
      "test mean loss=2293.7121880425348\n",
      "63\n",
      "train mean loss=37.18281684027778\n",
      "test mean loss=2255.1985134548613\n",
      "64\n",
      "train mean loss=42.71111979166667\n",
      "test mean loss=2218.6208116319444\n",
      "65\n",
      "train mean loss=39.326710069444445\n",
      "test mean loss=2181.5529730902776\n",
      "66\n",
      "train mean loss=34.01906684027778\n",
      "test mean loss=2146.485959201389\n",
      "67\n",
      "train mean loss=37.92083767361111\n",
      "test mean loss=2111.4762722439236\n",
      "68\n",
      "train mean loss=37.302235243055556\n",
      "test mean loss=2076.7837809244793\n",
      "69\n",
      "train mean loss=33.69114583333333\n",
      "test mean loss=2042.9439968532986\n",
      "70\n",
      "train mean loss=31.820911458333335\n",
      "test mean loss=2010.1891845703126\n",
      "71\n",
      "train mean loss=33.34361328125\n",
      "test mean loss=1977.79697265625\n",
      "72\n",
      "train mean loss=36.14771484375\n",
      "test mean loss=1945.4203450520833\n",
      "73\n",
      "train mean loss=34.82531032986111\n",
      "test mean loss=1913.900062391493\n",
      "74\n",
      "train mean loss=31.383059895833334\n",
      "test mean loss=1882.578724500868\n",
      "75\n",
      "train mean loss=34.31718967013889\n",
      "test mean loss=1851.3239637586805\n",
      "76\n",
      "train mean loss=31.617026909722224\n",
      "test mean loss=1820.9120686848958\n",
      "77\n",
      "train mean loss=29.1965625\n",
      "test mean loss=1789.0889675564235\n",
      "78\n",
      "train mean loss=30.058780381944445\n",
      "test mean loss=1747.3354193793402\n",
      "79\n",
      "train mean loss=30.275983072916667\n",
      "test mean loss=1709.2473958333333\n",
      "80\n",
      "train mean loss=27.073253038194444\n",
      "test mean loss=1675.090177408854\n",
      "81\n",
      "train mean loss=30.025145399305554\n",
      "test mean loss=1642.160706922743\n",
      "82\n",
      "train mean loss=28.825885416666665\n",
      "test mean loss=1610.5475775824652\n",
      "83\n",
      "train mean loss=26.20029079861111\n",
      "test mean loss=1579.530783420139\n",
      "84\n",
      "train mean loss=26.234368489583332\n",
      "test mean loss=1547.844637044271\n",
      "85\n",
      "train mean loss=24.600970052083333\n",
      "test mean loss=1515.711048719618\n",
      "86\n",
      "train mean loss=25.18426432291667\n",
      "test mean loss=1486.5060356987847\n",
      "87\n",
      "train mean loss=23.118721788194446\n",
      "test mean loss=1457.8691324869792\n",
      "88\n",
      "train mean loss=24.225080295138888\n",
      "test mean loss=1430.0058295355902\n",
      "89\n",
      "train mean loss=22.980785590277776\n",
      "test mean loss=1400.9015028211807\n",
      "90\n",
      "train mean loss=24.186671006944444\n",
      "test mean loss=1368.4172878689237\n",
      "91\n",
      "train mean loss=23.639346788194445\n",
      "test mean loss=1339.0036159939236\n",
      "92\n",
      "train mean loss=21.1183203125\n",
      "test mean loss=1311.6329264322917\n",
      "93\n",
      "train mean loss=22.10712890625\n",
      "test mean loss=1282.737703450521\n",
      "94\n",
      "train mean loss=21.400082465277777\n",
      "test mean loss=1256.872531467014\n",
      "95\n",
      "train mean loss=19.452235243055554\n",
      "test mean loss=1231.5414496527778\n",
      "96\n",
      "train mean loss=22.974680989583334\n",
      "test mean loss=1206.648228624132\n",
      "97\n",
      "train mean loss=19.467443576388888\n",
      "test mean loss=1182.7214545355903\n",
      "98\n",
      "train mean loss=20.82484809027778\n",
      "test mean loss=1160.820347764757\n",
      "99\n",
      "train mean loss=18.35529079861111\n",
      "test mean loss=1139.973055013021\n",
      "100\n",
      "train mean loss=18.35701388888889\n",
      "test mean loss=1119.8712727864583\n"
     ]
    }
   ],
   "source": [
    "c = 1\n",
    "while train_iter.epoch < epoch:\n",
    "    sum_loss = 0\n",
    "    batch = train_iter.next()\n",
    "    x_array_0 = convert.concat_examples(list(map(lambda x: x[0], batch)), gpu)\n",
    "    x_array_1 = convert.concat_examples(list(map(lambda x: x[1], batch)), gpu)\n",
    "    x = chainer.Variable(x_array_0)\n",
    "    # Update model based on the loss function\n",
    "    # defined by model.get_loss_func()\n",
    "    optimizer.update(model.get_loss_func(), x, x_array_1)\n",
    "    sum_loss += float(model.loss.data) * len(x.data)\n",
    "    if train_iter.is_new_epoch:\n",
    "        print(c)\n",
    "        c += 1\n",
    "        print('train mean loss={}'.format(sum_loss / train_count))\n",
    "        # evaluation\n",
    "        sum_loss = 0\n",
    "        for batch in val_iter:\n",
    "            x_array_0 = convert.concat_examples(list(map(lambda x: x[0], batch)), gpu)\n",
    "            x_array_1 = convert.concat_examples(list(map(lambda x: x[1], batch)), gpu)\n",
    "            x = chainer.Variable(x_array_0)\n",
    "            loss_func = model.get_loss_func(k=10)\n",
    "            loss_func(x, x_array_1)\n",
    "            sum_loss += float(model.loss.data) * len(x.data)\n",
    "        val_iter.reset()\n",
    "        print('val mean loss={}'.format(sum_loss / val_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the model\n",
      "save the optimizer\n"
     ]
    }
   ],
   "source": [
    "# Save the model and the optimizer\n",
    "# Check if folder exist \n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    \n",
    "print('save the model')\n",
    "chainer.serializers.save_npz(os.path.join(out, 'model.model'), model)\n",
    "print('save the optimizer')\n",
    "chainer.serializers.save_npz(os.path.join(out, 'state.state'), optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "def save_images(x, filename):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(3, 3, figsize=(9, 9), dpi=100)\n",
    "    for ai, xi in zip(ax.flatten(), x):\n",
    "        ai.imshow(xi[0])\n",
    "    fig.savefig(filename)\n",
    "    \n",
    "plt.gray()\n",
    "model.to_cpu()\n",
    "train_ind = np.random.randint(0,len(train),9)\n",
    "# train_ind = [0,1,2,3,4,5,6,7,8]\n",
    "batch = np.asarray(train)[train_ind]\n",
    "x_array1 = convert.concat_examples(list(map(lambda x: x[0], batch)), gpu)\n",
    "x_array2 = convert.concat_examples(list(map(lambda x: x[1], batch)), gpu)\n",
    "x = chainer.Variable(x_array1)\n",
    "with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "    x1 = model(x)\n",
    "save_images(x_array1, os.path.join(out, 'train_x'))\n",
    "save_images(x_array2, os.path.join(out, 'train_xnext'))\n",
    "save_images(x1.data, os.path.join(out, 'train_xnextreconstructed'))\n",
    "\n",
    "# test_ind = np.random.randint(0,len(test),9)\n",
    "test_ind = [0,1,2,3,4,5,6,7,8]\n",
    "batch = np.asarray(test)[test_ind]\n",
    "x_array1 = convert.concat_examples(list(map(lambda x: x[0], batch)), gpu)\n",
    "x_array2 = convert.concat_examples(list(map(lambda x: x[1], batch)), gpu)\n",
    "x = chainer.Variable(x_array1)\n",
    "with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "    x1 = model(x)\n",
    "save_images(x_array1, os.path.join(out, 'test_x'))\n",
    "save_images(x_array2, os.path.join(out, 'test_xnext'))\n",
    "save_images(x1.data, os.path.join(out, 'test_xnextreconstructed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can this model predict a Glider?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_g = hkl.load(PATH_glider + 'X_test.hkl')\n",
    "X_test_g = X_test_g / 255.\n",
    "X_test_g = X_test_g.astype('float32')\n",
    "sources_test_g = hkl.load(PATH_glider + 'sources_test.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of input data\n",
    "plt.imshow(X_test_g[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict glider data\n",
    "glider = list()\n",
    "for idx in range(len(X_test_g)-1):\n",
    "    if sources_test_g[idx] == sources_test_g[idx+1]:\n",
    "        glider.append((np.transpose(X_test_g[idx], (2,0,1)), np.transpose(X_test_g[idx+1], (2,0,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "def save_images2(x, filename):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(5, 5, figsize=(9, 9), dpi=100)\n",
    "    for ai, xi in zip(ax.flatten(), x):\n",
    "        ai.imshow(xi[0])\n",
    "    fig.savefig(filename)\n",
    "    \n",
    "glider_ind = range(25)#[0,1,2,3,4,5,6,7,8]\n",
    "batch = np.asarray(glider)[glider_ind]\n",
    "x_array1 = convert.concat_examples(list(map(lambda x: x[0], batch)), gpu)\n",
    "x_array2 = convert.concat_examples(list(map(lambda x: x[1], batch)), gpu)\n",
    "x = chainer.Variable(x_array1)\n",
    "with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "    x1 = model(x)\n",
    "save_images2(x_array1, os.path.join(out, 'glider_x'))\n",
    "save_images2(x_array2, os.path.join(out, 'glider_xnext'))\n",
    "save_images2(x1.data, os.path.join(out, 'glider_xnextreconstructed'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for paper (same format as Prednet output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ind = [0,1,2,3,4,5,6,7,8,9]\n",
    "test_ind = list(np.linspace(0, 99, 100, dtype='int32'))\n",
    "batch = np.asarray(test_for_plot)[test_ind]\n",
    "# test_ind = list(np.linspace(0, 9, 10, dtype='int32'))\n",
    "# batch = np.asarray(test_for_plot_glider)[test_ind]\n",
    "x_array_1 = convert.concat_examples(list(map(lambda x: x, batch)), gpu)\n",
    "x = chainer.Variable(x_array_1)\n",
    "with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "    x1 = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array_1.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = x_array_1.transpose(0,2,3,1).reshape(int(x_array1.data.shape[0]/10), 10, input_shape_0, input_shape_1, 3)\n",
    "X_hat = x1.data.transpose(0,2,3,1).reshape(int(input_shape_0.data.shape[0]/10), 10, input_shape_0, input_shape_1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.gray()\n",
    "nt = 10\n",
    "aspect_ratio = 0.8\n",
    "# plt.figure(figsize = (nt, 2*aspect_ratio))\n",
    "plt.figure(figsize = (100/7.2, 16/7.2))\n",
    "gs = gridspec.GridSpec(2, nt)\n",
    "gs.update(wspace=0., hspace=0.)\n",
    "plot_save_dir = 'fig_for_paper/gol_large/'\n",
    "if not os.path.exists(plot_save_dir): os.mkdir(plot_save_dir)\n",
    "plot_idx = np.random.permutation(X_test.shape[0])\n",
    "\n",
    "for i in plot_idx:\n",
    "    for t in range(nt):     \n",
    "        plt.subplot(gs[t])\n",
    "        plt.imshow(X_test[i,t,:16,:20,:], interpolation='none')\n",
    "        plt.tick_params(axis='both', which='both', bottom='off', top='off', left='off', right='off', labelbottom='off', labelleft='off')\n",
    "        if t==0: plt.ylabel('Actual', fontsize=10)\n",
    "\n",
    "        plt.subplot(gs[t + nt])\n",
    "        if t % 10 == 0:\n",
    "            plt.imshow(np.zeros(X_hat[0,0,:16,:20,:].shape), interpolation='none')\n",
    "        else:\n",
    "            plt.imshow(X_hat[i,t-1,:16,:20,:], interpolation='none')\n",
    "        plt.tick_params(axis='both', which='both', bottom='off', top='off', left='off', right='off', labelbottom='off', labelleft='off')\n",
    "        if t==0: plt.ylabel('Predicted', fontsize=10)\n",
    "    \n",
    "    plt.savefig(plot_save_dir +  'plot_' + str(i) + '.png')\n",
    "    plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_model = 0\n",
    "mse_prev = 0\n",
    "c = 0\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(len(X_test[0])-1):\n",
    "        mse_model += np.mean((X_test[i,j+1,:,:] - X_hat[i,j,:,:])**2)\n",
    "        mse_prev += np.mean((X_test[i,j+1,:,:] - X_test[i,j,:,:])**2)        \n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_model/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_prev/c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
