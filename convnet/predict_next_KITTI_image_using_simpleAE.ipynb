{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import os\n",
    "import chainer\n",
    "from chainer.dataset import convert\n",
    "import cv2\n",
    "import net_conv\n",
    "import hickle as hkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data input-output variables\n",
    "# path to dataset hkl files\n",
    "PATH = 'D:/ShareData/kitti_data/kitti_hkl/'\n",
    "# output path\n",
    "out = 'results/kitti'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training, validation and test data\n",
    "X_train = hkl.load(PATH + 'X_train.hkl')\n",
    "X_train = X_train / 255.\n",
    "X_train = X_train.astype('float32')\n",
    "sources_train = hkl.load(PATH + 'sources_train.hkl')\n",
    "\n",
    "X_val = hkl.load(PATH + 'X_val.hkl')\n",
    "X_val = X_val / 255.\n",
    "X_val = X_val.astype('float32')\n",
    "sources_val = hkl.load(PATH + 'sources_val.hkl')\n",
    "\n",
    "X_test = hkl.load(PATH + 'X_test.hkl')\n",
    "X_test = X_test / 255.\n",
    "X_test = X_test.astype('float32')\n",
    "sources_test = hkl.load(PATH + 'sources_test.hkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape_0 = X_train[0].shape[0]\n",
    "input_shape_1 = X_train[0].shape[1]\n",
    "gpu = -1\n",
    "dimz = 2\n",
    "batchsize = 16\n",
    "epoch = 20\n",
    "initmodel = ''\n",
    "resume = ''\n",
    "is_test = False #'store_true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPU: {}'.format(gpu))\n",
    "print('# dim z: {}'.format(dimz))\n",
    "print('# Minibatch-size: {}'.format(batchsize))\n",
    "print('# epoch: {}'.format(epoch))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize / Resume\n",
    "if initmodel:\n",
    "    chainer.serializers.load_npz(initmodel, model)\n",
    "if resume:\n",
    "    chainer.serializers.load_npz(resume, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list()\n",
    "for idx in range(len(X_train)-1):\n",
    "    if sources_train[idx] == sources_train[idx+1]:\n",
    "        train.append((np.transpose(X_train[idx], (2,0,1)), np.transpose(X_train[idx+1], (2,0,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = list()\n",
    "for idx in range(len(X_val)-1):\n",
    "    if sources_val[idx] == sources_val[idx+1]:\n",
    "        val.append((np.transpose(X_val[idx], (2,0,1)), np.transpose(X_val[idx+1], (2,0,1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list()\n",
    "for idx in range(len(X_test)-1):\n",
    "    test.append(np.transpose(X_test[idx], (2,0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train)\n",
    "random.shuffle(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_test:\n",
    "    train, _ = chainer.datasets.split_dataset(train, 100)\n",
    "    val, _ = chainer.datasets.split_dataset(val, 100)\n",
    "train_count = len(train)\n",
    "val_count = len(val)\n",
    "\n",
    "train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "val_iter = chainer.iterators.SerialIterator(val, batchsize, repeat=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net_conv.ConvAE_mini(input_shape_0=input_shape_0, input_shape_1=input_shape_1, channel_size=3, n_filters=10, n_latent=dimz, filter_size=3, activation='relu')\n",
    "\n",
    "# Setup an optimizer\n",
    "optimizer = chainer.optimizers.Adam()\n",
    "optimizer.setup(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c = 1\n",
    "while train_iter.epoch < epoch:\n",
    "    sum_loss = 0\n",
    "    batch = train_iter.next()\n",
    "    x_array_0 = convert.concat_examples(list(map(lambda x: x[0], batch)), gpu)\n",
    "    x_array_1 = convert.concat_examples(list(map(lambda x: x[1], batch)), gpu)\n",
    "    x = chainer.Variable(x_array_0)\n",
    "    # Update model based on the loss function\n",
    "    # defined by model.get_loss_func()\n",
    "    optimizer.update(model.get_loss_func(), x, x_array_1)\n",
    "    sum_loss += float(model.loss.data) * len(x.data)\n",
    "    if train_iter.is_new_epoch:\n",
    "        print(c)\n",
    "        c += 1\n",
    "        print('train mean loss={}'.format(sum_loss / train_count))\n",
    "        # evaluation\n",
    "        sum_loss = 0\n",
    "        for batch in val_iter:\n",
    "            x_array_0 = convert.concat_examples(list(map(lambda x: x[0], batch)), gpu)\n",
    "            x_array_1 = convert.concat_examples(list(map(lambda x: x[1], batch)), gpu)\n",
    "            x = chainer.Variable(x_array_0)\n",
    "            loss_func = model.get_loss_func(k=10)\n",
    "            loss_func(x, x_array_1)\n",
    "            sum_loss += float(model.loss.data) * len(x.data)\n",
    "        val_iter.reset()\n",
    "        print('val mean loss={}'.format(sum_loss / val_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and the optimizer\n",
    "# Check if folder exist \n",
    "if not os.path.exists(out):\n",
    "    os.makedirs(out)\n",
    "    print('created ', out)\n",
    "    \n",
    "print('save the model')\n",
    "chainer.serializers.save_npz(os.path.join(out, 'model.model'), model)\n",
    "print('save the optimizer')\n",
    "chainer.serializers.save_npz(os.path.join(out, 'state.state'), optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "def save_images(x, filename):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig, ax = plt.subplots(3, 3, figsize=(9, 9), dpi=100)\n",
    "    for ai, xi in zip(ax.flatten(), x):\n",
    "        ai.imshow(np.transpose(xi, (1,2,0)))\n",
    "    fig.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.to_cpu()\n",
    "train_ind = np.random.randint(0,len(train),9)\n",
    "# train_ind = [0,1,2,3,4,5,6,7,8]\n",
    "batch = np.asarray(train)[train_ind]\n",
    "x_array_0 = convert.concat_examples(list(map(lambda x: x[0], batch)), gpu)\n",
    "x_array_1 = convert.concat_examples(list(map(lambda x: x[1], batch)), gpu)\n",
    "x = chainer.Variable(x_array_0)\n",
    "\n",
    "with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "    x1 = model(x)\n",
    "save_images(x_array_0, os.path.join(out, 'train_x'))\n",
    "save_images(x_array_1, os.path.join(out, 'train_xnext'))\n",
    "save_images(x1.data, os.path.join(out, 'train_xnext_reconstructed'))\n",
    "\n",
    "# test_ind = np.random.randint(0,len(test),9)\n",
    "val_ind = [0,1,2,3,4,5,6,7,8]\n",
    "batch = np.asarray(val)[val_ind]\n",
    "x_array_0 = convert.concat_examples(list(map(lambda x: x[0], batch)), gpu)\n",
    "x_array_1 = convert.concat_examples(list(map(lambda x: x[1], batch)), gpu)\n",
    "x = chainer.Variable(x_array_0)\n",
    "with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "    x1 = model(x)\n",
    "save_images(x_array_0, os.path.join(out, 'train_x'))\n",
    "save_images(x_array_1, os.path.join(out, 'train_xnext'))\n",
    "save_images(x1.data, os.path.join(out, 'train_xnext_reconstructed'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot for paper (same format with Prednet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib.use('Agg')\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ind = [0,1,2,3,4,5,6,7,8,9]\n",
    "test_ind = list(np.linspace(0, len(test)-2, len(test)-1, dtype='int32'))\n",
    "batch = np.asarray(test)[test_ind]\n",
    "# test_ind = list(np.linspace(0, 9, 10, dtype='int32'))\n",
    "# batch = np.asarray(test_glider)[test_ind]\n",
    "x_array_0 = convert.concat_examples(list(map(lambda x: x, batch)), gpu)\n",
    "x = chainer.Variable(x_array_0)\n",
    "with chainer.using_config('train', False), chainer.no_backprop_mode():\n",
    "    x1 = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_array_0.transpose(0,2,3,1).shape)\n",
    "print(x1.data.transpose(0,2,3,1).shape)\n",
    "print(x_array_0[:10].data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = x_array_0.transpose(0,2,3,1).reshape(int(x_array_0.data.shape[0]/10), 10, input_shape_0, input_shape_1, 3)\n",
    "X_hat = x1.data.transpose(0,2,3,1).reshape(int(x_array_0.data.shape[0]/10), 10, input_shape_0, input_shape_1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nt = 10\n",
    "aspect_ratio = 0.8\n",
    "# plt.figure(figsize = (nt, 2*aspect_ratio))\n",
    "plt.figure(figsize = (100/7.2, 16/7.2))\n",
    "gs = gridspec.GridSpec(2, nt)\n",
    "gs.update(wspace=0., hspace=0.)\n",
    "plot_save_dir = 'fig_for_paper/KITTI/'\n",
    "\n",
    "if not os.path.exists(plot_save_dir): \n",
    "    os.makedirs(plot_save_dir)\n",
    "    print(\"created\", plot_save_dir)\n",
    "    \n",
    "plot_idx = np.random.permutation(X_test.shape[0])\n",
    "\n",
    "for i in plot_idx:\n",
    "    for t in range(nt):     \n",
    "        plt.subplot(gs[t])\n",
    "        plt.imshow(X_test[i,t], interpolation='none')\n",
    "        plt.tick_params(axis='both', which='both', bottom='off', top='off', left='off', right='off', labelbottom='off', labelleft='off')\n",
    "        if t==0: plt.ylabel('Actual', fontsize=10)\n",
    "\n",
    "        plt.subplot(gs[t + nt])\n",
    "        if t % 10 == 0:\n",
    "            plt.imshow(np.zeros(X_hat[0,0].shape), interpolation='none')\n",
    "        else:\n",
    "            plt.imshow(X_hat[i,t-1], interpolation='none')\n",
    "        plt.tick_params(axis='both', which='both', bottom='off', top='off', left='off', right='off', labelbottom='off', labelleft='off')\n",
    "        if t==0: plt.ylabel('Predicted', fontsize=10)\n",
    "    \n",
    "    plt.savefig(plot_save_dir +  'plot_' + str(i) + '.png')\n",
    "    plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_model = 0\n",
    "mse_prev = 0\n",
    "c = 0\n",
    "for i in range(len(X_test)):\n",
    "    for j in range(len(X_test[0])-1):\n",
    "        mse_model += np.mean((X_test[i,j+1,:,:] - X_hat[i,j,:,:])**2)\n",
    "        mse_prev += np.mean((X_test[i,j+1,:,:] - X_test[i,j,:,:])**2)        \n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_model/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_prev/c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
